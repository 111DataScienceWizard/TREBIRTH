{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/111DataScienceWizard/TREBIRTH/blob/main/Collab%20Notes/Binary_Classification_SKLearn_models_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing Necessary libraries**"
      ],
      "metadata": {
        "id": "xbGeI78vg-3a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5M3bD8Vahek",
        "outputId": "65ea0a1c-73bf-4447-c801-5d1f8924af06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.2)\n"
          ]
        }
      ],
      "source": [
        " !pip install onnx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install skl2onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvGwq9SdauAO",
        "outputId": "ecd2e054-2f2b-4239-e20c-22da495d00bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: skl2onnx in /usr/local/lib/python3.10/dist-packages (1.16.0)\n",
            "Requirement already satisfied: onnx>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from skl2onnx) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.10/dist-packages (from skl2onnx) (1.2.2)\n",
            "Requirement already satisfied: onnxconverter-common>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from skl2onnx) (1.14.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx>=1.2.1->skl2onnx) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.2.1->skl2onnx) (3.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxconverter-common>=1.7.0->skl2onnx) (23.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19->skl2onnx) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19->skl2onnx) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19->skl2onnx) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WATKNiL9avpl",
        "outputId": "792e52bd-2255-4067-a095-9b77062d966b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.16.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf2onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IOncs2fayU1",
        "outputId": "0151c601-9f5a-4f4d-ac1d-a6a03cd610f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tf2onnx in /usr/local/lib/python3.10/dist-packages (1.16.1)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.23.5)\n",
            "Requirement already satisfied: onnx>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (23.5.26)\n",
            "Requirement already satisfied: protobuf~=3.20 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Necessary Libraries**"
      ],
      "metadata": {
        "id": "QmDYtTCRhLN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import onnx\n",
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import FloatTensorType\n",
        "import onnxruntime as ort\n",
        "from tf2onnx import convert\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "Z7omrb21a0ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_labels(df, label):\n",
        "    df['label'] = label\n",
        "    return df"
      ],
      "metadata": {
        "id": "IRlNhNyPa38G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    columns_to_keep = ['Radar ADC', 'LSM Magnitude']\n",
        "    return data[columns_to_keep]"
      ],
      "metadata": {
        "id": "QET6LXbUrdQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-xlKFjebA0G",
        "outputId": "e842b2e6-6978-4a37-8a49-965e8128a292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading train data"
      ],
      "metadata": {
        "id": "W3qCPmaxhRBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "healthy_data_1 = add_labels(pd.read_excel('/content/drive/MyDrive/New Version/HealthyS1_ButtonTop.xlsx'), 'healthy')\n",
        "healthy_data_2 = add_labels(pd.read_excel('/content/drive/MyDrive/New Version/HealthyS2_ButtonRight.xlsx'), 'healthy')\n",
        "healthy_data_3 = add_labels(pd.read_excel('/content/drive/MyDrive/New Version/HealthyS3_ButtonBottom.xlsx'), 'healthy')"
      ],
      "metadata": {
        "id": "JAqkr-krbDWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infected_data_1 = add_labels(pd.read_excel('/content/drive/MyDrive/New Version/Handheld_I1.xlsx'), 'Infected')\n",
        "infected_data_2 = add_labels(pd.read_excel('/content/drive/MyDrive/New Version/Handheld_I2.xlsx'), 'Infected')\n",
        "infected_data_3 = add_labels(pd.read_excel('/content/drive/MyDrive/New Version/Handheld_I3.xlsx'), 'Infected')"
      ],
      "metadata": {
        "id": "FCLqX6aQbNLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenating data"
      ],
      "metadata": {
        "id": "N2b0N1jjhUWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "healthy_data = pd.concat([healthy_data_1, healthy_data_2, healthy_data_3])\n",
        "infected_data= pd.concat([infected_data_1, infected_data_2, infected_data_3])"
      ],
      "metadata": {
        "id": "mAgveg4XbO8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "healthy_data = preprocess_data(healthy_data)\n",
        "infected_data = preprocess_data(infected_data)"
      ],
      "metadata": {
        "id": "MmGCC6ipr5To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_columns1 = healthy_data.select_dtypes(include=np.number).columns\n",
        "numeric_columns2 = infected_data.select_dtypes(include=np.number).columns"
      ],
      "metadata": {
        "id": "8Zev3Q24bQwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading test data"
      ],
      "metadata": {
        "id": "EpBUhM3xhYbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h_d1 = add_labels(pd.read_excel('/content/drive/MyDrive/New Version/HealthyS4_ButtonLeft.xlsx'), 'healthy')"
      ],
      "metadata": {
        "id": "btWGyP-dVEaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i_d1 = add_labels(pd.read_excel('/content/drive/MyDrive/New Version/Handheld_I4.xlsx'), 'Infected')\n",
        "i_d2 = add_labels(pd.read_excel('/content/drive/MyDrive/New Version/Handheld_I5.xlsx'), 'Infected')"
      ],
      "metadata": {
        "id": "ZZh03VYHVEWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenating test data"
      ],
      "metadata": {
        "id": "YTXZNeH-ha15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h_d = pd.concat([h_d1])\n",
        "i_d= pd.concat([i_d1, i_d2])"
      ],
      "metadata": {
        "id": "BfsLqcOPVEUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_d = preprocess_data(h_d)\n",
        "i_d = preprocess_data(i_d)"
      ],
      "metadata": {
        "id": "7k7gsU5JsBDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_columns4 = h_d.select_dtypes(include=np.number).columns\n",
        "numeric_columns5 = i_d.select_dtypes(include=np.number).columns"
      ],
      "metadata": {
        "id": "VvlVZfUwVe1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "defining rollingstatisticsextractor functions in a class"
      ],
      "metadata": {
        "id": "66WiVdkohefw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This defines a class RollingStatisticsExtractor that inherits from BaseEstimator and TransformerMixin. It's meant to be used as part of a scikit-learn pipeline for data preprocessing."
      ],
      "metadata": {
        "id": "YNKYNGA_mJwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classes BaseEstimator and TransformerMixin are part of the scikit-learn library and are used for creating custom transformers in scikit-learn pipelines.\n",
        "\n",
        "BaseEstimator:\n",
        "\n",
        "BaseEstimator is the base class for all estimators in scikit-learn.\n",
        "An estimator in scikit-learn is any object that learns from data. It may be a classification algorithm, a regression algorithm, or a transformer that extracts features or preprocesses data.\n",
        "Inheriting from BaseEstimator ensures that your custom class complies with scikit-learn's conventions, making it compatible with various tools in the scikit-learn ecosystem.\n",
        "TransformerMixin:\n",
        "\n",
        "TransformerMixin is another base class in scikit-learn that extends BaseEstimator.\n",
        "When your class inherits from TransformerMixin, it gains additional functionality related to transformers, specifically the fit_transform method.\n",
        "TransformerMixin provides a default implementation of the fit_transform method based on the fit and transform methods. This can be convenient when creating custom transformers."
      ],
      "metadata": {
        "id": "NwBR1K4pm2ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RollingStatisticsExtractor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, window_size):\n",
        "        self.window_size = window_size\n",
        "        self.feature_names_out_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        numeric_columns = X.select_dtypes(include=np.number).columns\n",
        "        rolling_statistics = X[numeric_columns].rolling(window=self.window_size, min_periods=5)\n",
        "        aggregated_features = pd.DataFrame()\n",
        "\n",
        "        for column in numeric_columns:\n",
        "            aggregated_features[f'{column}_mean'] = rolling_statistics[column].mean()\n",
        "            aggregated_features[f'{column}_median'] = rolling_statistics[column].median()\n",
        "            aggregated_features[f'{column}_std'] = rolling_statistics[column].std()\n",
        "            aggregated_features[f'{column}_rms'] = rolling_statistics[column].apply(\n",
        "                lambda x: np.sqrt(np.mean(x ** 2)))\n",
        "            aggregated_features[f'{column}_peak2peak'] = rolling_statistics[column].max() - rolling_statistics[column].min()\n",
        "\n",
        "        # Replace NaN values with the mean of the corresponding column\n",
        "        aggregated_features = aggregated_features.fillna(aggregated_features.mean())\n",
        "\n",
        "        # Set feature names for this transformer\n",
        "        self.feature_names_out_ = aggregated_features.columns.tolist()\n",
        "        return aggregated_features\n",
        "\n",
        "    def fit_transform(self, X, y=None, **fit_params):\n",
        "        return self.fit(X).transform(X)\n",
        "\n",
        "    def get_feature_names_out(self, input_features=None):\n",
        "        return self.feature_names_out_"
      ],
      "metadata": {
        "id": "siTrGnq-iOLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "defining functions for EDA"
      ],
      "metadata": {
        "id": "Xk31FS7Mhp_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create an instance of the EDATransformer\n",
        "eda_transformer = EDATransformer()\n",
        "\n",
        "# Apply the EDA transformation\n",
        "transformed_data = eda_transformer.fit_transform(your_data)"
      ],
      "metadata": {
        "id": "tvfVEKhOn6g6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EDATransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        print(\"Data Information:\")\n",
        "        print(X.info())\n",
        "\n",
        "        print(\"\\nData head:\")\n",
        "        print(X.head())\n",
        "\n",
        "        print(\"\\nData Columns:\")\n",
        "        print(X.columns)\n",
        "\n",
        "        print(\"\\nData Shape:\")\n",
        "        print(X.shape)\n",
        "\n",
        "        print(\"\\nData types:\")\n",
        "        print(X.dtypes)\n",
        "\n",
        "        print(\"\\nData Summary Statistics:\")\n",
        "        print(X.describe())\n",
        "\n",
        "        print(\"\\nMissing Values:\")\n",
        "        print(X.isnull().sum())\n",
        "\n",
        "        print(\"\\nUnique Values:\")\n",
        "        for column in X.columns:\n",
        "            print(f\"{column}: {X[column].nunique()} unique values\")\n",
        "\n",
        "        print(\"\\nValue Counts:\")\n",
        "        for column in X.columns:\n",
        "            print(f\"{column}:\\n{X[column].value_counts()}\\n\")\n",
        "\n",
        "        print(\"\\nCorrelation Map:\")\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(X.corr(), annot=True, cmap='coolwarm', linewidths=.5)\n",
        "        plt.title('Correlation Map')\n",
        "        plt.show()\n",
        "\n",
        "        # Additional steps\n",
        "        print(\"\\nPerforming EDA on Aggregated Features:\")\n",
        "        transform(X)\n",
        "\n",
        "        cleaned_data = X.dropna()\n",
        "        for i in X.columns:\n",
        "            print(i)\n",
        "            print(X[i].nunique())\n",
        "            print(X[i].unique())\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "aX2DCQBQiCuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "numerical columns"
      ],
      "metadata": {
        "id": "EK-x7_3qhz06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_columns_healthy = healthy_data[numeric_columns1].select_dtypes(include=np.number).columns\n",
        "numeric_columns_infected = infected_data[numeric_columns2].select_dtypes(include=np.number).columns\n",
        "numeric_columns_h_t = h_d[numeric_columns4].select_dtypes(include=np.number).columns\n",
        "numeric_columns_i_t = i_d[numeric_columns5].select_dtypes(include=np.number).columns\n"
      ],
      "metadata": {
        "id": "0z_ue_MUXNAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting features"
      ],
      "metadata": {
        "id": "oZZU3_1Uh5g3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('rolling_stats_healthy', RollingStatisticsExtractor(window_size=500), numeric_columns_healthy),\n",
        "        ('rolling_stats_infected', RollingStatisticsExtractor(window_size=500), numeric_columns_infected),\n",
        "        ('rolling_stats_h_t', RollingStatisticsExtractor(window_size=500), numeric_columns_h_t),\n",
        "        ('rolling_stats_i_t', RollingStatisticsExtractor(window_size=500), numeric_columns_i_t)\n",
        "    ],\n",
        "    remainder='passthrough'  # Include non-numeric columns as they are\n",
        ")"
      ],
      "metadata": {
        "id": "cIhzUr1aT8gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ASSigning pipeline to each dataset and creating dataframe with the transferred data and column names"
      ],
      "metadata": {
        "id": "aYjAk2fEh8RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the transformers using the get_params method\n",
        "healthy_transformer = preprocessor.get_params()['rolling_stats_healthy']\n",
        "infected_transformer = preprocessor.get_params()['rolling_stats_infected']\n",
        "\n",
        "# Create separate pipelines for healthy and infected data\n",
        "healthy_pipeline = Pipeline([\n",
        "    ('preprocessor', healthy_transformer)\n",
        "])\n",
        "\n",
        "infected_pipeline = Pipeline([\n",
        "    ('preprocessor', infected_transformer)\n",
        "])\n",
        "\n",
        "# Fit and transform the healthy data\n",
        "healthy_data_with_features_array = healthy_pipeline['preprocessor'].fit_transform(healthy_data)\n",
        "\n",
        "# Get column names from the fitted transformer\n",
        "column_names_healthy = healthy_pipeline['preprocessor'].get_feature_names_out()\n",
        "\n",
        "# Create DataFrame with the transformed data and column names\n",
        "healthy_data_with_features = pd.DataFrame(healthy_data_with_features_array, columns=column_names_healthy)\n",
        "\n",
        "# Fit and transform the infected data\n",
        "infected_data_with_features_array = infected_pipeline['preprocessor'].fit_transform(infected_data)\n",
        "\n",
        "# Get column names from the fitted transformer\n",
        "column_names_infected = infected_pipeline['preprocessor'].get_feature_names_out()\n",
        "\n",
        "# Create DataFrame with the transformed data and column names\n",
        "infected_data_with_features = pd.DataFrame(infected_data_with_features_array, columns=column_names_infected)\n"
      ],
      "metadata": {
        "id": "Sa0N1-3_SdN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "combining orginal features with extracted features and giving labels, train and test assigning"
      ],
      "metadata": {
        "id": "KaBsCG-HiNCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `healthy_data` and `infected_data` contain the original features\n",
        "# Combine the original features with the extracted features for healthy data\n",
        "healthy_data_with_combined_features = pd.concat([healthy_data, healthy_data_with_features], axis=1)\n",
        "\n",
        "# Add labels to the combined healthy data\n",
        "healthy_data_with_combined_features['label'] = 0  # Healthy label is 0\n",
        "\n",
        "# Combine the original features with the extracted features for infected data\n",
        "infected_data_with_combined_features = pd.concat([infected_data.reset_index(drop=True), infected_data_with_features.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Add labels to the combined infected data\n",
        "infected_data_with_combined_features['label'] = 1  # Infected label is 1\n",
        "\n",
        "# Concatenate healthy and infected data\n",
        "train_data = pd.concat([healthy_data_with_combined_features, infected_data_with_combined_features], ignore_index=True, axis=0)\n",
        "\n",
        "# Print the columns of the combined_data\n",
        "print(train_data.columns)\n",
        "\n",
        "# Specify selected features\n",
        "selected_features = ['LSM Magnitude_mean', 'LSM Magnitude_median', 'LSM Magnitude_rms', 'LSM Magnitude', 'Radar ADC_peak2peak',]\n",
        "# Split features (X_combined) and labels (y_combined) for the combined data\n",
        "\n",
        "X_train = train_data[selected_features]\n",
        "y_train = train_data['label']\n",
        "\n",
        "# Print the first few rows of X_combined and y_combined\n",
        "print(X_train.head())\n",
        "print(y_train.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aDmh6xCbE0j",
        "outputId": "648c78be-1e1c-43c2-e9ea-1888592d0559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Radar ADC', 'LSM Magnitude', 'Radar ADC_mean', 'Radar ADC_median',\n",
            "       'Radar ADC_std', 'Radar ADC_rms', 'Radar ADC_peak2peak',\n",
            "       'LSM Magnitude_mean', 'LSM Magnitude_median', 'LSM Magnitude_std',\n",
            "       'LSM Magnitude_rms', 'LSM Magnitude_peak2peak', 'label'],\n",
            "      dtype='object')\n",
            "   LSM Magnitude_mean  LSM Magnitude_median  LSM Magnitude_rms  LSM Magnitude  \\\n",
            "0            1.003203              1.003151           1.003207       0.995339   \n",
            "1            1.003203              1.003151           1.003207       0.995339   \n",
            "2            1.003203              1.003151           1.003207       0.995339   \n",
            "3            1.003203              1.003151           1.003207       0.995339   \n",
            "4            0.995506              0.995339           0.995506       0.996173   \n",
            "\n",
            "   Radar ADC_peak2peak  \n",
            "0           408.671412  \n",
            "1           408.671412  \n",
            "2           408.671412  \n",
            "3           408.671412  \n",
            "4            21.000000  \n",
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ASSigning pipeline to each dataset and creating dataframe with the transferred data and column names"
      ],
      "metadata": {
        "id": "-eq98VLLiY0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the transformers using the get_params method\n",
        "h_transformer = preprocessor.get_params()['rolling_stats_h_t']\n",
        "i_transformer = preprocessor.get_params()['rolling_stats_i_t']\n",
        "\n",
        "# Create separate pipelines for healthy and infected data\n",
        "h_pipeline = Pipeline([\n",
        "    ('preprocessor', h_transformer)\n",
        "])\n",
        "\n",
        "i_pipeline = Pipeline([\n",
        "    ('preprocessor', i_transformer)\n",
        "])\n",
        "\n",
        "# Fit and transform the healthy data\n",
        "healthy_data_with_features_array = h_pipeline['preprocessor'].fit_transform(h_d)\n",
        "\n",
        "# Get column names from the fitted transformer\n",
        "column_names_healthy = h_pipeline['preprocessor'].get_feature_names_out()\n",
        "\n",
        "# Create DataFrame with the transformed data and column names\n",
        "healthy_data_with_features = pd.DataFrame(healthy_data_with_features_array, columns=column_names_healthy)\n",
        "\n",
        "# Fit and transform the infected data\n",
        "infected_data_with_features_array = i_pipeline['preprocessor'].fit_transform(i_d)\n",
        "\n",
        "# Get column names from the fitted transformer\n",
        "column_names_infected = infected_pipeline['preprocessor'].get_feature_names_out()\n",
        "\n",
        "# Create DataFrame with the transformed data and column names\n",
        "infected_data_with_features = pd.DataFrame(infected_data_with_features_array, columns=column_names_infected)\n"
      ],
      "metadata": {
        "id": "IWS2wfyPeh2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "combining orginal features with extracted features and giving labels, train and test assigning"
      ],
      "metadata": {
        "id": "RsUYfdxtind_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `healthy_data` and `infected_data` contain the original features\n",
        "# Combine the original features with the extracted features for healthy data\n",
        "healthy_data_with_combined_features = pd.concat([h_d, healthy_data_with_features], axis=1)\n",
        "\n",
        "# Add labels to the combined healthy data\n",
        "healthy_data_with_combined_features['label'] = 0  # Healthy label is 0\n",
        "\n",
        "# Combine the original features with the extracted features for infected data\n",
        "infected_data_with_combined_features = pd.concat([i_d.reset_index(drop=True), infected_data_with_features.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Add labels to the combined infected data\n",
        "infected_data_with_combined_features['label'] = 1  # Infected label is 1\n",
        "\n",
        "# Concatenate healthy and infected data\n",
        "test_data = pd.concat([healthy_data_with_combined_features, infected_data_with_combined_features], ignore_index=True, axis=0)\n",
        "\n",
        "# Print the columns of the combined_data\n",
        "print(test_data.columns)\n",
        "\n",
        "# Specify selected features\n",
        "selected_features = ['LSM Magnitude_mean', 'LSM Magnitude_median', 'LSM Magnitude_rms', 'LSM Magnitude', 'Radar ADC_peak2peak',]\n",
        "\n",
        "# Split features (X_combined) and labels (y_combined) for the combined data\n",
        "X_test = test_data[selected_features]\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Print the first few rows of X_combined and y_combined\n",
        "print(X_test.head())\n",
        "print(y_test.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPDWq2dmehln",
        "outputId": "8dfa182f-0a7e-4897-a44a-21c7caebd2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Radar ADC', 'LSM Magnitude', 'Radar ADC_mean', 'Radar ADC_median',\n",
            "       'Radar ADC_std', 'Radar ADC_rms', 'Radar ADC_peak2peak',\n",
            "       'LSM Magnitude_mean', 'LSM Magnitude_median', 'LSM Magnitude_std',\n",
            "       'LSM Magnitude_rms', 'LSM Magnitude_peak2peak', 'label'],\n",
            "      dtype='object')\n",
            "   LSM Magnitude_mean  LSM Magnitude_median  LSM Magnitude_rms  LSM Magnitude  \\\n",
            "0            0.979301              0.979219           0.979304       0.982328   \n",
            "1            0.979301              0.979219           0.979304       0.982328   \n",
            "2            0.979301              0.979219           0.979304       0.978678   \n",
            "3            0.979301              0.979219           0.979304       0.978678   \n",
            "4            0.980138              0.978678           0.980140       0.978678   \n",
            "\n",
            "   Radar ADC_peak2peak  \n",
            "0          1061.072688  \n",
            "1          1061.072688  \n",
            "2          1061.072688  \n",
            "3          1061.072688  \n",
            "4           118.000000  \n",
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "k0l7GNdrptM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "smote to balance imbalanced data"
      ],
      "metadata": {
        "id": "3SxvQl9xiuJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_imputed, y_train)"
      ],
      "metadata": {
        "id": "akENCqCeq1bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), selected_features)\n",
        "    ])"
      ],
      "metadata": {
        "id": "KCmxU8O1q1L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "defining function to train and evaluate the models in pipeline"
      ],
      "metadata": {
        "id": "VPfGP9m1izoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model, X_train_resampled, y_train_resampled, X_test, y_test, threshold=0.5):\n",
        "    full_pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    full_pipeline.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "    # For Training Set\n",
        "    y_train_pred_probs = full_pipeline.predict_proba(X_train_resampled)\n",
        "    print(f\"{type(model).__name__} Training Predicted Probabilities:\\n{y_train_pred_probs}\")\n",
        "\n",
        "    y_train_pred = (y_train_pred_probs[:, 1] > threshold).astype(int)  # Convert to class labels using threshold\n",
        "    print(f\"{type(model).__name__} Training Accuracy: {accuracy_score(y_train_resampled, y_train_pred)}\")\n",
        "    print(f\"{type(model).__name__} Training Classification Report:\\n{classification_report(y_train_resampled, y_train_pred)}\")\n",
        "\n",
        "    # For Test Set\n",
        "    y_test_pred_probs = full_pipeline.predict_proba(X_test)\n",
        "    print(f\"{type(model).__name__} Test Predicted Probabilities:\\n{y_test_pred_probs}\")\n",
        "\n",
        "    y_test_pred = (y_test_pred_probs[:, 1] > threshold).astype(int)  # Convert to class labels using threshold\n",
        "    print(f\"{type(model).__name__} Test Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
        "    print(f\"{type(model).__name__} Test Classification Report:\\n{classification_report(y_test, y_test_pred)}\")\n",
        "\n",
        "    return full_pipeline\n"
      ],
      "metadata": {
        "id": "FEU6yb4ZWTpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest pipeline training and evaluation"
      ],
      "metadata": {
        "id": "NqddDEEWjDBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)"
      ],
      "metadata": {
        "id": "PdWDTUF42RF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert NumPy arrays to pandas DataFrames\n",
        "X_train_resampled_df = pd.DataFrame(X_train_resampled, columns=selected_features)\n",
        "X_test_df = pd.DataFrame(X_test, columns=selected_features)\n",
        "\n",
        "# Assuming you have your labels as 1D arrays or lists\n",
        "y_train_resampled_df = pd.Series(y_train_resampled, name='label')\n",
        "y_test_df = pd.Series(y_test, name='label')\n",
        "\n",
        "# Now, you can use these DataFrames in your function\n",
        "rf_pipeline = train_and_evaluate(rf_model, X_train_resampled_df, y_train_resampled_df, X_test_df, y_test_df, threshold=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiQt7yDvXOG_",
        "outputId": "61d1ea7d-4b76-4ff4-b006-6fcc8dc580cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier Training Predicted Probabilities:\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " ...\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "RandomForestClassifier Training Accuracy: 1.0\n",
            "RandomForestClassifier Training Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     10314\n",
            "           1       1.00      1.00      1.00     10314\n",
            "\n",
            "    accuracy                           1.00     20628\n",
            "   macro avg       1.00      1.00      1.00     20628\n",
            "weighted avg       1.00      1.00      1.00     20628\n",
            "\n",
            "RandomForestClassifier Test Predicted Probabilities:\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "RandomForestClassifier Test Accuracy: 0.6970997528937443\n",
            "RandomForestClassifier Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      2329\n",
            "           1       0.70      1.00      0.82      5360\n",
            "\n",
            "    accuracy                           0.70      7689\n",
            "   macro avg       0.35      0.50      0.41      7689\n",
            "weighted avg       0.49      0.70      0.57      7689\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "logistic regression pipeline training and evaluation"
      ],
      "metadata": {
        "id": "7hm7P6C9jLX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logreg_pipeline = train_and_evaluate(logreg_model, X_train_resampled_df, y_train_resampled_df, X_test_df, y_test_df, threshold=0.5)\n"
      ],
      "metadata": {
        "id": "p6G84OhBh1XU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e61183-4c9f-4848-f689-1e63cb678600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression Training Predicted Probabilities:\n",
            "[[0.92795393 0.07204607]\n",
            " [0.92795393 0.07204607]\n",
            " [0.92795393 0.07204607]\n",
            " ...\n",
            " [0.99237014 0.00762986]\n",
            " [0.97744221 0.02255779]\n",
            " [0.98819249 0.01180751]]\n",
            "LogisticRegression Training Accuracy: 0.9290285049447353\n",
            "LogisticRegression Training Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.93     10314\n",
            "           1       0.93      0.92      0.93     10314\n",
            "\n",
            "    accuracy                           0.93     20628\n",
            "   macro avg       0.93      0.93      0.93     20628\n",
            "weighted avg       0.93      0.93      0.93     20628\n",
            "\n",
            "LogisticRegression Test Predicted Probabilities:\n",
            "[[0.05476728 0.94523272]\n",
            " [0.05476728 0.94523272]\n",
            " [0.04146013 0.95853987]\n",
            " ...\n",
            " [0.03886936 0.96113064]\n",
            " [0.03891066 0.96108934]\n",
            " [0.038952   0.961048  ]]\n",
            "LogisticRegression Test Accuracy: 0.7062036675770581\n",
            "LogisticRegression Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.04      0.07      2329\n",
            "           1       0.70      1.00      0.83      5360\n",
            "\n",
            "    accuracy                           0.71      7689\n",
            "   macro avg       0.77      0.52      0.45      7689\n",
            "weighted avg       0.75      0.71      0.60      7689\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "decision tree pipeline training and evaluation"
      ],
      "metadata": {
        "id": "DLB6aR9NjQn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_pipeline = train_and_evaluate(dt_model, X_train_resampled_df, y_train_resampled_df, X_test_df, y_test_df, threshold=0.5)\n"
      ],
      "metadata": {
        "id": "EO4SaRhAlSrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c0057fd-1d21-4750-b4ac-54c50a4838c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTreeClassifier Training Predicted Probabilities:\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " ...\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "DecisionTreeClassifier Training Accuracy: 1.0\n",
            "DecisionTreeClassifier Training Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     10314\n",
            "           1       1.00      1.00      1.00     10314\n",
            "\n",
            "    accuracy                           1.00     20628\n",
            "   macro avg       1.00      1.00      1.00     20628\n",
            "weighted avg       1.00      1.00      1.00     20628\n",
            "\n",
            "DecisionTreeClassifier Test Predicted Probabilities:\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "DecisionTreeClassifier Test Accuracy: 0.7893094030433087\n",
            "DecisionTreeClassifier Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.31      0.47      2329\n",
            "           1       0.77      1.00      0.87      5360\n",
            "\n",
            "    accuracy                           0.79      7689\n",
            "   macro avg       0.87      0.65      0.67      7689\n",
            "weighted avg       0.83      0.79      0.75      7689\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "svm pipeline training and evaluation"
      ],
      "metadata": {
        "id": "FBGLIIA7jTu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model = SVC(probability=True, random_state=42)\n",
        "svm_pipeline = train_and_evaluate(svm_model, X_train_resampled_df, y_train_resampled_df, X_test_df, y_test_df, threshold=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI1oXuLSrol0",
        "outputId": "fe236ae5-0b15-41f0-8950-702acc01917c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVC Training Predicted Probabilities:\n",
            "[[9.99989465e-01 1.05348079e-05]\n",
            " [9.99989465e-01 1.05348079e-05]\n",
            " [9.99989465e-01 1.05348079e-05]\n",
            " ...\n",
            " [9.98523533e-01 1.47646679e-03]\n",
            " [9.99535811e-01 4.64189483e-04]\n",
            " [9.99793783e-01 2.06216978e-04]]\n",
            "SVC Training Accuracy: 0.9794454140003879\n",
            "SVC Training Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     10314\n",
            "           1       0.97      0.99      0.98     10314\n",
            "\n",
            "    accuracy                           0.98     20628\n",
            "   macro avg       0.98      0.98      0.98     20628\n",
            "weighted avg       0.98      0.98      0.98     20628\n",
            "\n",
            "SVC Test Predicted Probabilities:\n",
            "[[0.06265009 0.93734991]\n",
            " [0.06265009 0.93734991]\n",
            " [0.06470434 0.93529566]\n",
            " ...\n",
            " [0.03443122 0.96556878]\n",
            " [0.03443557 0.96556443]\n",
            " [0.03443994 0.96556006]]\n",
            "SVC Test Accuracy: 0.6970997528937443\n",
            "SVC Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      2329\n",
            "           1       0.70      1.00      0.82      5360\n",
            "\n",
            "    accuracy                           0.70      7689\n",
            "   macro avg       0.35      0.50      0.41      7689\n",
            "weighted avg       0.49      0.70      0.57      7689\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "installing onxx necessary libraries"
      ],
      "metadata": {
        "id": "Y_q9BU-sjZYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onnxruntime-tools\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-xLM3AQrrNZ",
        "outputId": "2cdca20c-28b5-4c6b-f983-d40a7f6a025a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime-tools\n",
            "  Downloading onnxruntime_tools-1.7.0-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (1.23.5)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (15.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (9.0.0)\n",
            "Collecting py3nvml (from onnxruntime-tools)\n",
            "  Downloading py3nvml-0.2.7-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (23.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime-tools) (10.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->onnxruntime-tools) (3.20.2)\n",
            "Collecting xmltodict (from py3nvml->onnxruntime-tools)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict, py3nvml, onnxruntime-tools\n",
            "Successfully installed onnxruntime-tools-1.7.0 py3nvml-0.2.7 xmltodict-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skl2onnx.common.data_types import FloatTensorType"
      ],
      "metadata": {
        "id": "u3d8mRgmr6gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "defining functions to convert models to onxx"
      ],
      "metadata": {
        "id": "uEYTSzg0jgDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_onnx(model, X, onnx_path, input_names=None):\n",
        "    if input_names is None:\n",
        "        input_names = list(X.columns)\n",
        "\n",
        "    # Convert input names to ONNX-compatible format\n",
        "    initial_type = [(name, FloatTensorType([None, 1])) for name in input_names]\n",
        "\n",
        "    onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
        "\n",
        "    with open(onnx_path, 'wb') as f:\n",
        "        f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "c5e8LoISr_CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting random forest pipeline to onxx and ort"
      ],
      "metadata": {
        "id": "woXkGEEBjnkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with RandomForest model\n",
        "convert_to_onnx(rf_pipeline, X_test_df, 'rf_model.onnx', input_names=selected_features)\n",
        "rf_ort_model = ort.InferenceSession('rf_model.onnx')"
      ],
      "metadata": {
        "id": "QZMYn6gZsCDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting logistic regression pipeline to onxx and ort"
      ],
      "metadata": {
        "id": "hiWdh7SWjy1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_onnx(logreg_pipeline, X_test_df, 'logreg_model.onnx', input_names=selected_features)\n",
        "logreg_ort_model = ort.InferenceSession('logreg_model.onnx')"
      ],
      "metadata": {
        "id": "VjtpBOAKsHBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting decision tree pipeline to onxx and ort"
      ],
      "metadata": {
        "id": "zPGaDNCPj2Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_onnx(dt_pipeline, X_test_df, 'dt_model.onnx', input_names=selected_features)\n",
        "dt_ort_model = ort.InferenceSession('dt_model.onnx')"
      ],
      "metadata": {
        "id": "VXF4nN-4sK0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting svm pipeline to onxx and ort"
      ],
      "metadata": {
        "id": "tTAdyP25j6RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_onnx(svm_pipeline, X_test_df, 'svm_model.onnx', input_names=selected_features)\n",
        "svm_ort_model = ort.InferenceSession('svm_model.onnx')"
      ],
      "metadata": {
        "id": "QWZk5dU8sNBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "defining function to evaluate onxx model"
      ],
      "metadata": {
        "id": "h3fXg4Jhj9_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "threshold = 0.5: This line sets a threshold value for converting predicted probabilities to predicted classes. If the predicted probability for a class is greater than this threshold, it will be considered as the predicted class; otherwise, it won't.\n",
        "\n",
        "input_names = [input.name for input in dt_ort_model.get_inputs()]: This line retrieves the names of the input nodes of the ONNX model (dt_ort_model). The ONNX model can have multiple inputs, and this line creates a list (input_names) containing the names of those inputs.\n",
        "\n",
        "X_test[selected_features].iloc[:, i].values.astype(np.float32)[:, None]: This part of the code extracts the relevant columns (selected_features) from the test data (X_test) and prepares them for input to the ONNX model. It converts the values to a NumPy array, changes the data type to float32, and adds an additional axis to make it suitable for input to the ONNX model.\n",
        "\n",
        "{input_names[i]: ... for i in range(len(input_names))}: This part of the code creates a dictionary that maps the input names of the ONNX model to the corresponding preprocessed test data. It uses a dictionary comprehension to iterate over the input names and their corresponding preprocessed data.\n",
        "\n",
        "dt_ort_model.run(None, ...)[0]: This line performs inference using the ONNX model (dt_ort_model). It takes the input data (dictionary) prepared in the previous step, runs the inference, and retrieves the output. The [0] is used because run returns a list of outputs, and we are assuming there is only one output in this case.\n",
        "\n",
        "So, in summary, this code sets a threshold, prepares the input data for the ONNX model using the test data, and then uses the ONNX model to make predictions on the test data. The output (dt_ort_predictions) contains the predicted probabilities for each class."
      ],
      "metadata": {
        "id": "3TdcJzFwpc1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_onnx_model(model, X_test, y_test, selected_features, threshold=0.5):\n",
        "    input_names = [input.name for input in model.get_inputs()]\n",
        "    print(f\"Expected input names for the model: {input_names}\")\n",
        "\n",
        "    # Get predicted probabilities using ONNX model\n",
        "    ort_predictions = model.run(None, {input_names[i]: X_test[selected_features].iloc[:, i].values.astype(np.float32)[:, None] for i in range(len(input_names))})[0]\n",
        "\n",
        "    # Check if the array has only one dimension\n",
        "    if len(ort_predictions.shape) == 1:\n",
        "        ort_predictions = ort_predictions[:, None]  # Add a second dimension\n",
        "\n",
        "    # Convert predicted probabilities to predicted classes using the threshold\n",
        "    ort_predicted_classes_prob = (ort_predictions > threshold).astype(int)\n",
        "\n",
        "    # Create a DataFrame to display true labels and predicted probabilities side by side\n",
        "    result_df = pd.DataFrame({\n",
        "        'True Label': y_test,\n",
        "        'Predicted Probability': ort_predictions.flatten(),\n",
        "        'Predicted Class': ort_predicted_classes_prob.flatten()\n",
        "    })\n",
        "\n",
        "    # Print the DataFrame\n",
        "    print(result_df)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    ort_accuracy_prob = accuracy_score(y_test, ort_predicted_classes_prob)\n",
        "    print(f\"\\nONNX Model Accuracy using Probabilities and Threshold: {ort_accuracy_prob}\")"
      ],
      "metadata": {
        "id": "p-hpTqD9sPEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "'True Label': This column in the DataFrame represents the actual labels from the test set (y_test). It contains the ground truth values.\n",
        "\n",
        "'Predicted Probability': This column represents the predicted probabilities generated by the machine learning model. The variable ort_predictions likely contains the raw predicted probabilities for each instance in the test set. The flatten() method is used to convert the multi-dimensional array into a one-dimensional array suitable for DataFrame construction.\n",
        "\n",
        "'Predicted Class': This column represents the predicted classes based on the predicted probabilities. It seems like ort_predicted_classes_prob contains the predicted classes, and flatten() is again used to convert the array to one-dimensional."
      ],
      "metadata": {
        "id": "B6_xVI6yq_Zc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "printing random forest ort pipeline predictions and accuracy"
      ],
      "metadata": {
        "id": "uGRsVhmekO2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_onnx_model(rf_ort_model, X_test_df, y_test_df, selected_features, threshold=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5iZUycgsU8w",
        "outputId": "a0c65a2f-e67f-49f0-8a00-cbb3c4f824b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected input names for the model: ['LSM_Magnitude_mean', 'LSM_Magnitude_median', 'LSM_Magnitude_rms', 'LSM_Magnitude', 'Radar_ADC_peak2peak']\n",
            "      True Label  Predicted Probability  Predicted Class\n",
            "0              0                      1                1\n",
            "1              0                      1                1\n",
            "2              0                      1                1\n",
            "3              0                      1                1\n",
            "4              0                      1                1\n",
            "...          ...                    ...              ...\n",
            "7684           1                      1                1\n",
            "7685           1                      1                1\n",
            "7686           1                      1                1\n",
            "7687           1                      1                1\n",
            "7688           1                      1                1\n",
            "\n",
            "[7689 rows x 3 columns]\n",
            "\n",
            "ONNX Model Accuracy using Probabilities and Threshold: 0.6970997528937443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "printing logistic regression ort pipeline predictions and accuracy"
      ],
      "metadata": {
        "id": "cogcpEQpkhxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_onnx_model(logreg_ort_model, X_test_df, y_test_df, selected_features, threshold=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuVUrdA0sY_b",
        "outputId": "54d30482-1c6f-4cf1-d0d0-961745b33b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected input names for the model: ['LSM_Magnitude_mean', 'LSM_Magnitude_median', 'LSM_Magnitude_rms', 'LSM_Magnitude', 'Radar_ADC_peak2peak']\n",
            "      True Label  Predicted Probability  Predicted Class\n",
            "0              0                      1                1\n",
            "1              0                      1                1\n",
            "2              0                      1                1\n",
            "3              0                      1                1\n",
            "4              0                      0                0\n",
            "...          ...                    ...              ...\n",
            "7684           1                      1                1\n",
            "7685           1                      1                1\n",
            "7686           1                      1                1\n",
            "7687           1                      1                1\n",
            "7688           1                      1                1\n",
            "\n",
            "[7689 rows x 3 columns]\n",
            "\n",
            "ONNX Model Accuracy using Probabilities and Threshold: 0.7062036675770581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "printing decision tree ort pipeline predictions and accuracy"
      ],
      "metadata": {
        "id": "JEjsP_zBkovY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_onnx_model(dt_ort_model, X_test_df, y_test_df, selected_features, threshold=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfgJEPSvsgO8",
        "outputId": "49457d82-3fd0-494d-e7f0-3d096d9f1b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected input names for the model: ['LSM_Magnitude_mean', 'LSM_Magnitude_median', 'LSM_Magnitude_rms', 'LSM_Magnitude', 'Radar_ADC_peak2peak']\n",
            "      True Label  Predicted Probability  Predicted Class\n",
            "0              0                      1                1\n",
            "1              0                      1                1\n",
            "2              0                      1                1\n",
            "3              0                      1                1\n",
            "4              0                      0                0\n",
            "...          ...                    ...              ...\n",
            "7684           1                      1                1\n",
            "7685           1                      1                1\n",
            "7686           1                      1                1\n",
            "7687           1                      1                1\n",
            "7688           1                      1                1\n",
            "\n",
            "[7689 rows x 3 columns]\n",
            "\n",
            "ONNX Model Accuracy using Probabilities and Threshold: 0.7893094030433087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "printing svm ort pipeline predictions and accuracy"
      ],
      "metadata": {
        "id": "0ZDTQ7gzktOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_onnx_model(svm_ort_model, X_test_df, y_test_df, selected_features, threshold=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5qwxEftsgcc",
        "outputId": "abc3fc3e-5bbd-4248-d17e-ea7d3739499b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected input names for the model: ['LSM_Magnitude_mean', 'LSM_Magnitude_median', 'LSM_Magnitude_rms', 'LSM_Magnitude', 'Radar_ADC_peak2peak']\n",
            "      True Label  Predicted Probability  Predicted Class\n",
            "0              0                      1                1\n",
            "1              0                      1                1\n",
            "2              0                      1                1\n",
            "3              0                      1                1\n",
            "4              0                      1                1\n",
            "...          ...                    ...              ...\n",
            "7684           1                      1                1\n",
            "7685           1                      1                1\n",
            "7686           1                      1                1\n",
            "7687           1                      1                1\n",
            "7688           1                      1                1\n",
            "\n",
            "[7689 rows x 3 columns]\n",
            "\n",
            "ONNX Model Accuracy using Probabilities and Threshold: 0.6970997528937443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-q8H5yPIsgrI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}